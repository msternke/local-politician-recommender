{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import twint\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/nyc_mayor_info_all.csv', 'r') as file:\n",
    "    mayor_df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>party</th>\n",
       "      <th>website</th>\n",
       "      <th>election_type</th>\n",
       "      <th>twitter_handles</th>\n",
       "      <th>curbed_contenter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Eric Adams</td>\n",
       "      <td>D</td>\n",
       "      <td>https://www.ericadams2021.com/</td>\n",
       "      <td>mayor</td>\n",
       "      <td>ericadamsfornyc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Shaun Donovan</td>\n",
       "      <td>D</td>\n",
       "      <td>https://www.shaunfornyc.com/</td>\n",
       "      <td>mayor</td>\n",
       "      <td>ShaunDonovanNYC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Barbara Kavovit</td>\n",
       "      <td>O</td>\n",
       "      <td>https://www.barbarakformayor.com</td>\n",
       "      <td>mayor</td>\n",
       "      <td>msbarbarak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Ray McGuire</td>\n",
       "      <td>D</td>\n",
       "      <td>https://www.rayformayor.com/</td>\n",
       "      <td>mayor</td>\n",
       "      <td>RayForMayor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Carlos Menchaca</td>\n",
       "      <td>D</td>\n",
       "      <td>https://carlos2021.com/</td>\n",
       "      <td>mayor</td>\n",
       "      <td>cmenchaca</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Dianne Morales</td>\n",
       "      <td>D</td>\n",
       "      <td>https://www.dianne.nyc/</td>\n",
       "      <td>mayor</td>\n",
       "      <td>Dianne4NYC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Curtis Sliwa</td>\n",
       "      <td>R</td>\n",
       "      <td>https://www.facebook.com/SliwaforNYC2021/</td>\n",
       "      <td>mayor</td>\n",
       "      <td>SliwaforNYC2021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Scott Stringer</td>\n",
       "      <td>D</td>\n",
       "      <td>https://stringerformayor.com/</td>\n",
       "      <td>mayor</td>\n",
       "      <td>scottmstringer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Loree Sutton</td>\n",
       "      <td>D</td>\n",
       "      <td>https://loreeformayor.nyc/</td>\n",
       "      <td>mayor</td>\n",
       "      <td>LoreeSuttonNYC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>Maya Wiley</td>\n",
       "      <td>D</td>\n",
       "      <td>https://mayawileyformayor.com/</td>\n",
       "      <td>mayor</td>\n",
       "      <td>mayawiley</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>Andrew Yang</td>\n",
       "      <td>D</td>\n",
       "      <td>https://www.yangforny.com/</td>\n",
       "      <td>mayor</td>\n",
       "      <td>AndrewYang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0             name party  \\\n",
       "0            0       Eric Adams     D   \n",
       "3            3    Shaun Donovan     D   \n",
       "13          13  Barbara Kavovit     O   \n",
       "16          16      Ray McGuire     D   \n",
       "17          17  Carlos Menchaca     D   \n",
       "18          18   Dianne Morales     D   \n",
       "23          23     Curtis Sliwa     R   \n",
       "24          24   Scott Stringer     D   \n",
       "25          25     Loree Sutton     D   \n",
       "29          29       Maya Wiley     D   \n",
       "31          31      Andrew Yang     D   \n",
       "\n",
       "                                      website election_type  twitter_handles  \\\n",
       "0              https://www.ericadams2021.com/         mayor  ericadamsfornyc   \n",
       "3                https://www.shaunfornyc.com/         mayor  ShaunDonovanNYC   \n",
       "13           https://www.barbarakformayor.com         mayor       msbarbarak   \n",
       "16               https://www.rayformayor.com/         mayor      RayForMayor   \n",
       "17                    https://carlos2021.com/         mayor        cmenchaca   \n",
       "18                    https://www.dianne.nyc/         mayor       Dianne4NYC   \n",
       "23  https://www.facebook.com/SliwaforNYC2021/         mayor  SliwaforNYC2021   \n",
       "24              https://stringerformayor.com/         mayor   scottmstringer   \n",
       "25                 https://loreeformayor.nyc/         mayor   LoreeSuttonNYC   \n",
       "29             https://mayawileyformayor.com/         mayor        mayawiley   \n",
       "31                 https://www.yangforny.com/         mayor       AndrewYang   \n",
       "\n",
       "    curbed_contenter  \n",
       "0                  1  \n",
       "3                  1  \n",
       "13                 1  \n",
       "16                 1  \n",
       "17                 1  \n",
       "18                 1  \n",
       "23                 1  \n",
       "24                 1  \n",
       "25                 1  \n",
       "29                 1  \n",
       "31                 1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mayor_df_contenders = mayor_df[mayor_df['curbed_contenter'] == 1]\n",
    "mayor_df_contenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Eric-Adams! Found 722 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Art-Chang! Found 300 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Eddie-Cullen! Found 32 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Shaun-Donovan! Found 415 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Thomas-Downs! Found 2 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Guiddalia-Emilien! Found 14 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Vitaly-Filipchenko! Found 33 Tweets.\n",
      "\n",
      "Done scrapeing Cleopatra-Fitzgerald! Found 3000 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Aaron-Foldenauer! Found 354 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Quanda-Francis! Found 253 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Kathryn-Garcia! Found 402 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Garry-Guerrier! Found 5 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Peter-Guimaraes! Found 2 Tweets.\n",
      "\n",
      "Done scrapeing Barbara-Kavovit! Found 3095 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Christopher-S.-Krietchman! Found 2035 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Abbey-Laurel-Smith! Found 2340 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Ray-McGuire! Found 295 Tweets.\n",
      "\n",
      "Done scrapeing Carlos-Menchaca! Found 3000 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Dianne-Morales! Found 2495 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Bill-Pepitone! Found 725 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Paperboy-Prince! Found 2118 Tweets.\n",
      "\n",
      "Done scrapeing Stacey-Prussman! Found 3000 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Stephen-Bishop-Seely! Found 5 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Curtis-Sliwa! Found 84 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Scott-Stringer! Found 1589 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Loree-Sutton! Found 468 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Ahsan-Syed! Found 110 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Joycelyn-Taylor! Found 2412 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Sara-Tirschwell! Found 23 Tweets.\n",
      "\n",
      "Done scrapeing Maya-Wiley! Found 3000 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Isaac-Wright-Jr.! Found 242 Tweets.\n",
      "\n",
      "Done scrapeing Andrew-Yang! Found 3000 Tweets.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_user(candidate, election):\n",
    "    name = candidate['name'].replace(' ', '-')\n",
    "    \n",
    "    c = twint.Config()\n",
    "    c.Hide_output = True\n",
    "    c.Username = candidate['twitter_handles']\n",
    "    c.Pandas = True\n",
    "    c.Limit = 3000\n",
    "    c.Filter_retweets = False\n",
    "    \n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    tweets_df = twint.storage.panda.Tweets_df\n",
    "    \n",
    "    columns_wanted = ['date', 'tweet']\n",
    "    tweets_df_filtered = tweets_df[columns_wanted]\n",
    "    \n",
    "    with open(f'data/scraped_twitter_profiles/nyc/{election}_election/{name}_tweets.pkl', 'wb') as file:\n",
    "        pickle.dump(tweets_df_filtered, file)\n",
    "        \n",
    "    print(f'Done scrapeing {name}! Found {len(tweets_df_filtered)} Tweets.\\n')\n",
    "    \n",
    "    return(tweets_df_filtered)\n",
    "\n",
    "all_dfs = [scrape_user(candidate[1], 'mayor') for candidate in mayor_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scrapeing joebiden!\n",
      "Done scrapeing AOC!\n",
      "Done scrapeing SpeakerPelosi!\n",
      "Done scrapeing barackobama!\n",
      "Done scrapeing SenatorCollins!\n",
      "Done scrapeing tedcruz!\n",
      "Done scrapeing McConnellPress!\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing realbencarson!\n"
     ]
    }
   ],
   "source": [
    "def scrape_user(username):\n",
    "    c = twint.Config()\n",
    "    c.Hide_output = True\n",
    "    c.Username = username\n",
    "    c.Pandas = True\n",
    "    c.Limit = 3000\n",
    "    c.Filter_retweets = False\n",
    "    \n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    tweets_df = twint.storage.panda.Tweets_df\n",
    "    \n",
    "    columns_wanted = ['date', 'tweet']\n",
    "    tweets_df_filtered = tweets_df[columns_wanted]\n",
    "    \n",
    "    with open(f'data/scraped_twitter_profiles/national/{username}_tweets.pkl', 'wb') as file:\n",
    "        pickle.dump(tweets_df_filtered, file)\n",
    "        \n",
    "    print(f'Done scrapeing {username}!')\n",
    "    \n",
    "    return(tweets_df_filtered)\n",
    "\n",
    "national_politicians = ['joebiden', 'AOC', 'SpeakerPelosi', 'barackobama', 'SenatorCollins', 'tedcruz', 'McConnellPress', 'realbencarson']\n",
    "\n",
    "all_dfs = [scrape_user(candidate) for candidate in national_politicians]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
