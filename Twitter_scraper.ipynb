{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import twint\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/nyc_mayor_info_all.csv', 'r') as file:\n",
    "    mayor_df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Eric Adams! Found 762 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Art Chang! Found 343 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Eddie Cullen! Found 38 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Shaun Donovan! Found 449 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Thomas Downs! Found 2 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Guiddalia Emilien! Found 14 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Vitaly Filipchenko! Found 51 Tweets.\n",
      "\n",
      "Done scrapeing Cleopatra Fitzgerald! Found 3000 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Aaron Foldenauer! Found 359 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Quanda Francis! Found 268 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Kathryn Garcia! Found 453 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Garry Guerrier! Found 5 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Peter Guimaraes! Found 2 Tweets.\n",
      "\n",
      "Done scrapeing Barbara Kavovit! Found 3093 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Christopher S. Krietchman! Found 2045 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Abbey Laurel Smith! Found 2342 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Ray McGuire! Found 354 Tweets.\n",
      "\n",
      "Done scrapeing Carlos Menchaca! Found 3000 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Dianne Morales! Found 2526 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Bill Pepitone! Found 745 Tweets.\n",
      "\n",
      "Done scrapeing Angelo Pinto! Found 3000 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Paperboy Prince! Found 2108 Tweets.\n",
      "\n",
      "Done scrapeing Stacey Prussman! Found 3000 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Stephen Bishop Seely! Found 5 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Ira Seidman! Found 63 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Curtis Sliwa! Found 91 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Scott Stringer! Found 1608 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Loree Sutton! Found 482 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Ahsan Syed! Found 110 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Joycelyn Taylor! Found 2480 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Sara Tirschwell! Found 29 Tweets.\n",
      "\n",
      "Done scrapeing Maya Wiley! Found 3000 Tweets.\n",
      "\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing Isaac Wright Jr.! Found 247 Tweets.\n",
      "\n",
      "Done scrapeing Andrew Yang! Found 3000 Tweets.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_user(candidate, election):\n",
    "    name = candidate['name']\n",
    "    \n",
    "    c = twint.Config()\n",
    "    c.Hide_output = True\n",
    "    c.Username = candidate['twitter_handles']\n",
    "    c.Pandas = True\n",
    "    c.Limit = 3000\n",
    "    c.Filter_retweets = False\n",
    "    \n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    tweets_df = twint.storage.panda.Tweets_df\n",
    "    \n",
    "    columns_wanted = ['date', 'tweet']\n",
    "    tweets_df_filtered = tweets_df[columns_wanted]\n",
    "    \n",
    "    with open(f\"data/scraped_twitter_profiles/nyc/{election}_election/{name.replace(' ', '-')}_tweets.pkl\", 'wb') as file:\n",
    "        pickle.dump(tweets_df_filtered, file)\n",
    "        \n",
    "    print(f'Done scrapeing {name}! Found {len(tweets_df_filtered)} Tweets.\\n')\n",
    "    \n",
    "    return(tweets_df_filtered)\n",
    "\n",
    "all_dfs = [scrape_user(candidate[1], 'mayor') for candidate in mayor_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scrapeing joebiden!\n",
      "Done scrapeing AOC!\n",
      "Done scrapeing SpeakerPelosi!\n",
      "Done scrapeing barackobama!\n",
      "Done scrapeing SenatorCollins!\n",
      "Done scrapeing tedcruz!\n",
      "Done scrapeing McConnellPress!\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "Done scrapeing realbencarson!\n"
     ]
    }
   ],
   "source": [
    "def scrape_user(username):\n",
    "    c = twint.Config()\n",
    "    c.Hide_output = True\n",
    "    c.Username = username\n",
    "    c.Pandas = True\n",
    "    c.Limit = 3000\n",
    "    c.Filter_retweets = False\n",
    "    \n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    tweets_df = twint.storage.panda.Tweets_df\n",
    "    \n",
    "    columns_wanted = ['date', 'tweet']\n",
    "    tweets_df_filtered = tweets_df[columns_wanted]\n",
    "    \n",
    "    with open(f'data/scraped_twitter_profiles/national/{username}_tweets.pkl', 'wb') as file:\n",
    "        pickle.dump(tweets_df_filtered, file)\n",
    "        \n",
    "    print(f'Done scrapeing {username}!')\n",
    "    \n",
    "    return(tweets_df_filtered)\n",
    "\n",
    "national_politicians = ['joebiden', 'AOC', 'SpeakerPelosi', 'barackobama', 'SenatorCollins', 'tedcruz', 'McConnellPress', 'realbencarson']\n",
    "\n",
    "all_dfs = [scrape_user(candidate) for candidate in national_politicians]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
