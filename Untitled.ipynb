{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries_of_tweets():\n",
    "    '''Creates a dictionary of tweets for each politician'''\n",
    "    \n",
    "    out_dict = {}\n",
    "    \n",
    "    for filename in glob.glob('data/*.pkl'):\n",
    "        politician_handle = filename.split('/')[1].split('_')[0]\n",
    "        with open(filename, 'rb') as file:    \n",
    "            tweet_df = pickle.load(file)\n",
    "            out_dict[politician_handle] = tweet_df['tweet'].drop_duplicates().to_frame()\n",
    "    return(out_dict)\n",
    "\n",
    "politician_dict = create_dictionaries_of_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_sentiment_analysis(tweet):\n",
    "    \n",
    "    #converts html to text\n",
    "    tweet = BeautifulSoup(tweet, 'lxml').text\n",
    "    \n",
    "    #removes links\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    #removes twitter usernames\n",
    "    tweet = re.sub(r'(\\s)?@\\w+', '', tweet)\n",
    "    \n",
    "    return(tweet)\n",
    "\n",
    "for df in politician_dict.values():\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: clean_tweet_sentiment_analysis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calc_tweet_sentiments(tweet):    \n",
    "    sentiment_scores = sentiment_analyzer.polarity_scores(tweet)\n",
    "            \n",
    "    return sentiment_scores\n",
    "\n",
    "for df in politician_dict.values():\n",
    "    df['sentiment_scores'] = df['tweet'].apply(lambda x: calc_tweet_sentiments(x))\n",
    "    \n",
    "for df in politician_dict.values():\n",
    "    df['compound_score'] = df['sentiment_scores'].apply(lambda x: x['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = ['rt', 'amp', 'u', 'w', 'im', 'live', 'must', 'join', 'tune', 'pm', 'et', \n",
    "                         'year', 'say', 'get', 'it']\n",
    "stop_words = custom_stop_words + stopwords.words('english')\n",
    "\n",
    "punctuation = string.punctuation + \"'\"\n",
    "\n",
    "def clean_tweet_tfidf(tweet):\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #removing punctuation\n",
    "    cleaned_tweet = re.sub('[%s]' % re.escape(punctuation), '', tweet)\n",
    "    \n",
    "    #removing emojis\n",
    "    cleaned_tweet = re.sub(emoji.get_emoji_regexp(), r\"\", cleaned_tweet)\n",
    "        \n",
    "    #remove stop words\n",
    "    cleaned_tweet = ' '.join([item for item in cleaned_tweet.split() if item not in stop_words])\n",
    "        \n",
    "    return(cleaned_tweet)\n",
    "\n",
    "for df in politician_dict.values():\n",
    "    df['cleaned_tweets'] = df['tweet'].apply(lambda x: clean_tweet_tfidf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>lemmatized_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know who else gets sworn in today? Jon Oss...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'comp...</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>know else gets sworn today jon ossoff reverend...</td>\n",
       "      <td>know else get sworn today jon ossoff reverend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awful and unacceptable.</td>\n",
       "      <td>{'neg': 0.857, 'neu': 0.143, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.7184</td>\n",
       "      <td>awful unacceptable</td>\n",
       "      <td>awful unacceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I enjoyed the Asian American Secret Service ag...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.633, 'pos': 0.367, 'comp...</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>enjoyed asian american secret service agent be...</td>\n",
       "      <td>enjoy asian american secret service agent behi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amanda Gorman.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>amanda gorman</td>\n",
       "      <td>amanda gorman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 46th President of the United States.  üëçüá∫üá∏üéâ</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.621, 'pos': 0.379, 'comp...</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>46th president united states</td>\n",
       "      <td>46th president united state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>Another 1.9 million unemployed - 25% of the wo...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>another 19 million unemployed 25 workforce work</td>\n",
       "      <td>another 19 million unemployed 25 workforce work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>the scale of the protests is enormous</td>\n",
       "      <td>{'neg': 0.241, 'neu': 0.759, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>scale protests enormous</td>\n",
       "      <td>scale protest enormous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Thank you Marshall!  We will get it out to peo...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'comp...</td>\n",
       "      <td>0.4738</td>\n",
       "      <td>thank marshall people</td>\n",
       "      <td>thank marshall people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>Thanks Erik. üëç</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'comp...</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>thanks erik</td>\n",
       "      <td>thanks erik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Call it ‚Äò912.‚Äô</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>call ‚Äò912‚Äô</td>\n",
       "      <td>call ‚Äò 912 ‚Äô</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2988 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  \\\n",
       "0     You know who else gets sworn in today? Jon Oss...   \n",
       "1                               Awful and unacceptable.   \n",
       "2     I enjoyed the Asian American Secret Service ag...   \n",
       "3                                        Amanda Gorman.   \n",
       "4      The 46th President of the United States.  üëçüá∫üá∏üéâ     \n",
       "...                                                 ...   \n",
       "2994  Another 1.9 million unemployed - 25% of the wo...   \n",
       "2995              the scale of the protests is enormous   \n",
       "2997  Thank you Marshall!  We will get it out to peo...   \n",
       "2998                                     Thanks Erik. üëç   \n",
       "2999                                     Call it ‚Äò912.‚Äô   \n",
       "\n",
       "                                       sentiment_scores  compound_score  \\\n",
       "0     {'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'comp...          0.5673   \n",
       "1     {'neg': 0.857, 'neu': 0.143, 'pos': 0.0, 'comp...         -0.7184   \n",
       "2     {'neg': 0.0, 'neu': 0.633, 'pos': 0.367, 'comp...          0.7003   \n",
       "3     {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "4     {'neg': 0.0, 'neu': 0.621, 'pos': 0.379, 'comp...          0.6705   \n",
       "...                                                 ...             ...   \n",
       "2994  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "2995  {'neg': 0.241, 'neu': 0.759, 'pos': 0.0, 'comp...         -0.2263   \n",
       "2997  {'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'comp...          0.4738   \n",
       "2998  {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'comp...          0.4404   \n",
       "2999  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n",
       "\n",
       "                                         cleaned_tweets  \\\n",
       "0     know else gets sworn today jon ossoff reverend...   \n",
       "1                                    awful unacceptable   \n",
       "2     enjoyed asian american secret service agent be...   \n",
       "3                                         amanda gorman   \n",
       "4                          46th president united states   \n",
       "...                                                 ...   \n",
       "2994    another 19 million unemployed 25 workforce work   \n",
       "2995                            scale protests enormous   \n",
       "2997                              thank marshall people   \n",
       "2998                                        thanks erik   \n",
       "2999                                         call ‚Äò912‚Äô   \n",
       "\n",
       "                                      lemmatized_tweets  \n",
       "0     know else get sworn today jon ossoff reverend ...  \n",
       "1                                    awful unacceptable  \n",
       "2     enjoy asian american secret service agent behi...  \n",
       "3                                         amanda gorman  \n",
       "4                           46th president united state  \n",
       "...                                                 ...  \n",
       "2994    another 19 million unemployed 25 workforce work  \n",
       "2995                             scale protest enormous  \n",
       "2997                              thank marshall people  \n",
       "2998                                        thanks erik  \n",
       "2999                                       call ‚Äò 912 ‚Äô  \n",
       "\n",
       "[2988 rows x 5 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politician_dict['AndrewYang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization function taken from Selva Prabhakaran's post on Machine Learning Plus\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_tweets(tweets):\n",
    "    #initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    #lemmatize the words\n",
    "    return [' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(tweet)]) for tweet in tweets]\n",
    "\n",
    "for df in politician_dict.values():\n",
    "    df['lemmatized_tweets'] = lemmatize_tweets(df['cleaned_tweets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thanks\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-b5fb3e455ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#return final, number_of_tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mget_politician_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolitician_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AndrewYang'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemmatized_tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AndrewYang'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-b5fb3e455ff4>\u001b[0m in \u001b[0;36mget_politician_vectors\u001b[0;34m(tweets, twitter_handle, top_words, top_words_amount)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_words_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mmini_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mtweets_for_this_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmini_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_for_this_word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "def get_politician_vectors(df, twitter_handle, n_words = 250):\n",
    "    \n",
    "    #create the document-term matrix\n",
    "    tfidf = TfidfVectorizer(stop_words = custom_stop_words, \n",
    "                            ngram_range = (1, 3), min_df = 5, max_df = .9, binary = True)\n",
    "    doc_word = tfidf.fit_transform(df['lemmatized_tweets'])\n",
    "    \n",
    "    word_scores = pd.DataFrame(doc_word.toarray(), columns = tfidf.get_feature_names()).sum()\n",
    "    \n",
    "    politician_word_vector = pd.DataFrame(word_scores).transpose()   \n",
    "    \n",
    "    top_words = word_scores.nlargest(n_words).index\n",
    "    for word in top_words:\n",
    "        \n",
    "     \n",
    "    #get sentiment for top words and add them to the dataframe\n",
    "    if top_words == True:\n",
    "        top_words = word_scores.nlargest(top_words_amount).index\n",
    "        for word in top_words:\n",
    "            mini_df = pd.DataFrame(result,columns=['text'])\n",
    "            tweets_for_this_word = mini_df[mini_df.text.str.contains(word)].text.to_list()\n",
    "            if len(tweets_for_this_word) != 0:\n",
    "                senti_df = create_sentiment_vectors(tweets_for_this_word,twitter_handle,word)\n",
    "                for column in senti_df.columns:\n",
    "                    final[column] = senti_df.at[twitter_handle,column]\n",
    "\n",
    "    #return final, number_of_tweets\n",
    "    \n",
    "get_politician_vectors(politician_dict['AndrewYang']['lemmatized_tweets'], 'AndrewYang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271.28020000000066"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sent = 0\n",
    "for i, row in politician_dict['AndrewYang'].iterrows():\n",
    "    if 'thank' in row['lemmatized_tweets']:\n",
    "        word_sent += row['compound_score']\n",
    "word_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politician_dict = {key: clean_tweets_tfidf(value).to_frame() for (key, value) in politician_dict.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
